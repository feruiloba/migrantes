# ------------------------------------- #
# STEP 2: Install required R packages
# Install and load the following packages: "twitteR", "RCurl", "RJSONIO", "stringr".
#install.packages("twitteR")
library(twitteR)
install.packages(c("twitteR", "RCurl", "RJSONIO", "stringr"))
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
api_key <- "UAY3TSU5V167dujJFXGgrkSCw"
api_secret <- "iFb7GTafb5jvk0YPkblY5bQIzOllDzMtFBuuECeA3BLz0UfR2Z"
token <- "156399649-ftSc9hBm2ML9ipQq8uW8RZIbVXKFkkZhv3bveUwN"
token_secret <- "wLz6KfSZ1DP8ZBO71qvhC4bC0xAeR7pDu0aBQZYcxfuXJ"
# creates the connection to Twitter's Search API.
setup_twitter_oauth(api_key, api_secret, token, token_secret)
?searchTwitter
### For now, use the function searchTwitter for the hashtags mentioned above, use n=500, lang="es, since='2018-02-19', until='2018-02-23'
migrantes  <- searchTwitter("migrantes", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
deportados <- searchTwitter("deportados", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
asilo <- searchTwitter("asilo", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
migrantes  <- searchTwitter("#migrantes", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
deportados <- searchTwitter("#deportados", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
asilo <- searchTwitter("#asilo", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
### For now, use the function searchTwitter for the hashtags mentioned above, use n=500, lang="es, since='2018-02-19', until='2018-02-23'
migrantes  <- searchTwitter("#migrantes", n=500, lang="es",since='2018-06-22', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
deportados <- searchTwitter("#deportados", n=500, lang="es",since='2018-06-22', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
asilo <- searchTwitter("#asilo", n=500, lang="es",since='2018-06-22', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
ned above, use n=500, lang="es, since='2018-02-19', until='2018-02-23'
migrantes  <- searchTwitter("#migrantes", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
deportados <- searchTwitter("#deportados", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
asilo <- searchTwitter("#asilo", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,1000km')
018-02-23'
migrantes  <- searchTwitter("#migrantes", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,1000mi')
deportados <- searchTwitter("#deportados", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,1000mi')
asilo <- searchTwitter("#asilo", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,1000mi')
asilo <- searchTwitter("#asilo", n=500, lang="es",since='2018-06-25', until='2018-06-29')
asilo
migrantes  <- searchTwitter("#migrantes", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,1000mi')
asilo <- searchTwitter("#asilo", n=500, lang="es",since='2018-01-01', until='2018-06-29')
asilo
deportados <- searchTwitter("#deportados", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,1000mi')
deportados <- searchTwitter("#deportados", n=500, lang="es",since='2018-06-25', until='2018-06-29')
deportados
migrantes  <- searchTwitter("migrantes", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441, -99.132637,100mi')
migrantes  <- searchTwitter("migrantes", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441,-99.132637,100mi')
migrantes
migrantes  <- searchTwitter("migrantes", n=500, lang="es",since='2018-06-25', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
2018-02-23'
migrantes  <- searchTwitter("migrantes", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
deportados <- searchTwitter("deportados", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
asilo <- searchTwitter("asilo", n=500, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
df_tweets_migrantes  <- twListToDF(migrantes)
df_tweets_deportados <- twListToDF(deportados)
df_tweets_asilo <- twListToDF(asilo)
colnames(df_tweets_migrantes)
df_tweets_migrantes[1:10,1]
df_tweets_deportados[1:10,1]
df_tweets_asilo[1:10,1]
library(tm)
install.packages("tm")
myCorpus <- Corpus(VectorSource(df_tweets_migrantes$text))
library(tm)
myCorpus <- Corpus(VectorSource(df_tweets_migrantes$text))
myCorpus
#### Now we are going to use generar expessions to clean the data.
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords, for example prepositions
myStopwords <-(stopwords('spanish'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
tdm <- TermDocumentMatrix(myCorpus)
dtm <- DocumentTermMatrix(myCorpus)
# inspect frequent words usng the function findFreqTerms
freq.terms <- findFreqTerms(tdm, lowfreq = 20)
### Tansform to matrix
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
library(RColorBrewer)
install.packages("RColorBrewer")
pal <- brewer.pal(9, "BuGn")[-(1:4)]
library(RColorBrewer)
pal <- brewer.pal(9, "BuGn")[-(1:4)]
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
setwd("C:\Users\t-ferui\Documents\NuevoNorte")
setwd("C://Users//t-ferui//Documents//NuevoNorte")
png(filename="word_migrantes.png")
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,
random.order = F, colors = pal)
dev.off()
myCorpus <- Corpus(VectorSource(df_tweets_deportados$text))
#### Now we are going to use generar expessions to clean the data.
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords, for example prepositions
myStopwords <-(stopwords('spanish'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#####
tdm <- TermDocumentMatrix(myCorpus)
dtm <- DocumentTermMatrix(myCorpus)
# inspect frequent words usng the function findFreqTerms
freq.terms <- findFreqTerms(tdm, lowfreq = 20)
### Tansform to matrix
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# We're a bout to create a wordcloud. First, we need to download some colors
library(RColorBrewer)
pal <- brewer.pal(9, "BuGn")[-(1:4)]
#### Word cloud
### Install package wordcloud
library(wordcloud)
### Plot for migrantes
setwd("C://Users//t-ferui//Documents//NuevoNorte")
png(filename="word_deportados.png")
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,
random.order = F, colors = pal)
dev.off()
myCorpus <- Corpus(VectorSource(df_tweets_asilo$text))
#### Now we are going to use generar expessions to clean the data.
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords, for example prepositions
myStopwords <-(stopwords('spanish'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#####
tdm <- TermDocumentMatrix(myCorpus)
dtm <- DocumentTermMatrix(myCorpus)
# inspect frequent words usng the function findFreqTerms
freq.terms <- findFreqTerms(tdm, lowfreq = 20)
### Tansform to matrix
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# We're a bout to create a wordcloud. First, we need to download some colors
library(RColorBrewer)
pal <- brewer.pal(9, "BuGn")[-(1:4)]
#### Word cloud
### Install package wordcloud
library(wordcloud)
### Plot for migrantes
setwd("C://Users//t-ferui//Documents//NuevoNorte")
png(filename="word_asilo.png")
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,
random.order = F, colors = pal)
dev.off()
install.packages(c("devtools", "sentiment"))
require(devtools)
install_github('sentiment140', 'okugami79')
library(sentiment)
sentiment('Ayudar migrantes')
sentiment('Los migrantes nos invaden')
sentiment(c('Odio mi trabajo', 'Me encanta mi trabajo'))
migrantes_sentiment <- sentiment(df_tweets_migrantes$text)
migrantes_sentiment <- sentiment(df_tweets_migrantes$text)
deportados_sentiment <- sentiment(df_tweets_deportados$text)
asilo_sentiment <- sentiment(df_tweets_asilo$text)
df_sentiment <- rbind(table(migrantes_sentiment$polarity),
table(deportados_sentiment$polarity), table(asilo_sentiment$polarity))
df_sentiment <- as.data.frame(df_sentiment)
df_sentiment <- rbind(table(migrantes_sentiment$polarity),
table(deportados_sentiment$polarity), table(asilo_sentiment$polarity))
df_sentiment
df_sentiment$index <- (df_sentiment$negative / df_sentiment$positive)
df_sentiment <- as.data.frame(df_sentiment)
df_sentiment$categoria <- c("migrantes", "deportados", "asilo")
df_sentiment$index <- (df_sentiment$negative / df_sentiment$positive)
plot_neg   <- ggplot(df_sentiment, aes(y=negative, x=categoria)) +
geom_bar(stat="identity", fill="red") + theme_bw() + ggtitle("Negative")
plot_pos   <- ggplot(df_sentiment, aes(y=positive, x=categoria)) +
geom_bar(stat="identity", fill="#009E73") + theme_bw() + ggtitle("Positive")
plot_index <- ggplot(df_sentiment, aes(y=index, x=categoria)) +
geom_bar(stat="identity", fill="navy") + theme_bw() + ggtitle("Polarization index")
install.packages("ggplot2")
plot_neg   <- ggplot(df_sentiment, aes(y=negative, x=categoria)) +
geom_bar(stat="identity", fill="red") + theme_bw() + ggtitle("Negative")
plot_pos   <- ggplot(df_sentiment, aes(y=positive, x=categoria)) +
geom_bar(stat="identity", fill="#009E73") + theme_bw() + ggtitle("Positive")
plot_index <- ggplot(df_sentiment, aes(y=index, x=categoria)) +
geom_bar(stat="identity", fill="navy") + theme_bw() + ggtitle("Polarization index")
library(ggplot2)
plot_neg   <- ggplot(df_sentiment, aes(y=negative, x=categoria)) +
geom_bar(stat="identity", fill="red") + theme_bw() + ggtitle("Negative")
plot_pos   <- ggplot(df_sentiment, aes(y=positive, x=categoria)) +
geom_bar(stat="identity", fill="#009E73") + theme_bw() + ggtitle("Positive")
plot_index <- ggplot(df_sentiment, aes(y=index, x=categoria)) +
geom_bar(stat="identity", fill="navy") + theme_bw() + ggtitle("Polarization index")
plot_neg
plot_pos
plot_neg
plot_index
tuser_psolalinde  <- getUser('@padresolalinde')
tuser_monarcas <- getUser('@casamonarcasc')
tuser_otrosdreams <- getUser('@OtrosDreams_ODA')
tuser_newcomienzos <- getUser('@NewComienzos')
tuser_psolalinde$friendsCount
tuser_monarcas$friendsCount
tuser_otrosdreams$friendsCount
tuser_newcomienzos$friendsCount
tuser_psolalinde$followersCount
tuser_monarcas$followersCount
tuser_otrosdreams$followersCount
tuser_newcomienzos$followersCount
ggsave(plot_neg, file="plot_neg.pdf")
ggsave(plot_pos, file="plot_pos.pdf")
ggsave(plot_index, file="plot_index.pdf")
migrantes  <- searchTwitter("migrantes", n=1000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
deportados <- searchTwitter("deportados", n=1000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
asilo <- searchTwitter("asilo", n=1000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
migrantes  <- searchTwitter("migrantes", n=10000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
deportados <- searchTwitter("deportados", n=10000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
asilo <- searchTwitter("asilo", n=10000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
migrantes  <- searchTwitter("migrantes", n=10000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
migrantes  <- searchTwitter("migrantes", n=10000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
migrantes  <- searchTwitter("migrantes", n=100, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
deportados <- searchTwitter("deportados", n=10000, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
migrantes  <- searchTwitter("migrantes", n=100, lang="es",since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000mi')
migrantes  <- searchTwitter("migrantes", n=100, lang="es", since='2018-01-01', until='2018-06-29', geocode='19.387441,-99.132637,1000km')
migrantes  <- searchTwitter("migrantes", n=100, lang="es", since="2018-01-01", until="2018-06-29", geocode="19.387441,-99.132637,1000km")
api_key <- "UAY3TSU5V167dujJFXGgrkSCw"
api_secret <- "iFb7GTafb5jvk0YPkblY5bQIzOllDzMtFBuuECeA3BLz0UfR2Z"
token <- "156399649-ftSc9hBm2ML9ipQq8uW8RZIbVXKFkkZhv3bveUwN"
token_secret <- "wLz6KfSZ1DP8ZBO71qvhC4bC0xAeR7pDu0aBQZYcxfuXJ"
setup_twitter_oauth(api_key, api_secret, token, token_secret)
migrantes  <- searchTwitter("migrantes", n=100, lang="es", since="2018-01-01", until="2018-06-29", geocode="19.387441,-99.132637,1000km")
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
setup_twitter_oauth(api_key, api_secret, token, token_secret)
?searchTwitter
migrantes  <- searchTwitter("migrantes", n=100, lang="es", since="2018-01-01", until="2018-06-29", geocode="19.387441,-99.132637,1000km")
